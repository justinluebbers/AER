Logging to ./AER/logs/CartPole/deepq/default_replay/300000/perfect_RM/bs48/d1/0/
--------------------------------------
| % time spent exploring  | 92       |
| episodes                | 100      |
| mean 100 episode reward | 22       |
| steps                   | 2.18e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 83       |
| episodes                | 200      |
| mean 100 episode reward | 27.3     |
| steps                   | 4.91e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 75       |
| episodes                | 300      |
| mean 100 episode reward | 25.9     |
| steps                   | 7.5e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 67       |
| episodes                | 400      |
| mean 100 episode reward | 23       |
| steps                   | 9.8e+03  |
--------------------------------------
Saving model due to mean reward increase: None -> 24.100000381469727
--------------------------------------
| % time spent exploring  | 57       |
| episodes                | 500      |
| mean 100 episode reward | 33.5     |
| steps                   | 1.32e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 40       |
| episodes                | 600      |
| mean 100 episode reward | 49.3     |
| steps                   | 1.81e+04 |
--------------------------------------
Saving model due to mean reward increase: 24.100000381469727 -> 34.5
--------------------------------------
| % time spent exploring  | 27       |
| episodes                | 700      |
| mean 100 episode reward | 42.2     |
| steps                   | 2.23e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 12       |
| episodes                | 800      |
| mean 100 episode reward | 43.9     |
| steps                   | 2.67e+04 |
--------------------------------------
Saving model due to mean reward increase: 34.5 -> 66.5999984741211
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 900      |
| mean 100 episode reward | 118      |
| steps                   | 3.84e+04 |
--------------------------------------
Saving model due to mean reward increase: 66.5999984741211 -> 78.69999694824219
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1e+03    |
| mean 100 episode reward | 91.7     |
| steps                   | 4.76e+04 |
--------------------------------------
Saving model due to mean reward increase: 78.69999694824219 -> 103.4000015258789
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.1e+03  |
| mean 100 episode reward | 122      |
| steps                   | 5.98e+04 |
--------------------------------------
Saving model due to mean reward increase: 103.4000015258789 -> 121.4000015258789
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.2e+03  |
| mean 100 episode reward | 114      |
| steps                   | 7.11e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.3e+03  |
| mean 100 episode reward | 64.3     |
| steps                   | 7.76e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.4e+03  |
| mean 100 episode reward | 104      |
| steps                   | 8.8e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.5e+03  |
| mean 100 episode reward | 92.8     |
| steps                   | 9.73e+04 |
--------------------------------------
Saving model due to mean reward increase: 121.4000015258789 -> 141.6999969482422
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.6e+03  |
| mean 100 episode reward | 134      |
| steps                   | 1.11e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.7e+03  |
| mean 100 episode reward | 80.1     |
| steps                   | 1.19e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.8e+03  |
| mean 100 episode reward | 102      |
| steps                   | 1.29e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.9e+03  |
| mean 100 episode reward | 40.3     |
| steps                   | 1.33e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2e+03    |
| mean 100 episode reward | 120      |
| steps                   | 1.45e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.1e+03  |
| mean 100 episode reward | 118      |
| steps                   | 1.57e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.2e+03  |
| mean 100 episode reward | 17.8     |
| steps                   | 1.59e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.3e+03  |
| mean 100 episode reward | 10.9     |
| steps                   | 1.6e+05  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.4e+03  |
| mean 100 episode reward | 10.1     |
| steps                   | 1.61e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.5e+03  |
| mean 100 episode reward | 102      |
| steps                   | 1.71e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.6e+03  |
| mean 100 episode reward | 127      |
| steps                   | 1.84e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.7e+03  |
| mean 100 episode reward | 129      |
| steps                   | 1.97e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.8e+03  |
| mean 100 episode reward | 134      |
| steps                   | 2.1e+05  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.9e+03  |
| mean 100 episode reward | 47.9     |
| steps                   | 2.15e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3e+03    |
| mean 100 episode reward | 141      |
| steps                   | 2.29e+05 |
--------------------------------------
Saving model due to mean reward increase: 141.6999969482422 -> 144.1999969482422
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.1e+03  |
| mean 100 episode reward | 127      |
| steps                   | 2.42e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.2e+03  |
| mean 100 episode reward | 134      |
| steps                   | 2.55e+05 |
--------------------------------------
Saving model due to mean reward increase: 144.1999969482422 -> 147.8000030517578
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.3e+03  |
| mean 100 episode reward | 143      |
| steps                   | 2.69e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.4e+03  |
| mean 100 episode reward | 130      |
| steps                   | 2.82e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.5e+03  |
| mean 100 episode reward | 169      |
| steps                   | 2.99e+05 |
--------------------------------------
Restored model with mean reward: 147.8000030517578
