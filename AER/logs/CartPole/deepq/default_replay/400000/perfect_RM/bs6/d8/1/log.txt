Logging to ./AER/logs/CartPole/deepq/default_replay/400000/perfect_RM/bs6/d8/1/
--------------------------------------
| % time spent exploring  | 94       |
| episodes                | 100      |
| mean 100 episode reward | 23.3     |
| steps                   | 2.3e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 87       |
| episodes                | 200      |
| mean 100 episode reward | 27.7     |
| steps                   | 5.07e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 80       |
| episodes                | 300      |
| mean 100 episode reward | 29.6     |
| steps                   | 8.03e+03 |
--------------------------------------
Saving model due to mean reward increase: None -> 25.100000381469727
--------------------------------------
| % time spent exploring  | 73       |
| episodes                | 400      |
| mean 100 episode reward | 26.1     |
| steps                   | 1.06e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 64       |
| episodes                | 500      |
| mean 100 episode reward | 38.9     |
| steps                   | 1.45e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 57       |
| episodes                | 600      |
| mean 100 episode reward | 27.9     |
| steps                   | 1.73e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 51       |
| episodes                | 700      |
| mean 100 episode reward | 25.7     |
| steps                   | 1.99e+04 |
--------------------------------------
Saving model due to mean reward increase: 25.100000381469727 -> 25.700000762939453
--------------------------------------
| % time spent exploring  | 37       |
| episodes                | 800      |
| mean 100 episode reward | 55.3     |
| steps                   | 2.54e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 30       |
| episodes                | 900      |
| mean 100 episode reward | 28.3     |
| steps                   | 2.82e+04 |
--------------------------------------
Saving model due to mean reward increase: 25.700000762939453 -> 29.899999618530273
--------------------------------------
| % time spent exploring  | 21       |
| episodes                | 1e+03    |
| mean 100 episode reward | 38.1     |
| steps                   | 3.21e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 9        |
| episodes                | 1.1e+03  |
| mean 100 episode reward | 47.8     |
| steps                   | 3.68e+04 |
--------------------------------------
Saving model due to mean reward increase: 29.899999618530273 -> 62.400001525878906
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.2e+03  |
| mean 100 episode reward | 96.2     |
| steps                   | 4.64e+04 |
--------------------------------------
Saving model due to mean reward increase: 62.400001525878906 -> 102.0
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.3e+03  |
| mean 100 episode reward | 124      |
| steps                   | 5.89e+04 |
--------------------------------------
Saving model due to mean reward increase: 102.0 -> 122.5999984741211
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.4e+03  |
| mean 100 episode reward | 106      |
| steps                   | 6.95e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.5e+03  |
| mean 100 episode reward | 148      |
| steps                   | 8.42e+04 |
--------------------------------------
Saving model due to mean reward increase: 122.5999984741211 -> 169.60000610351562
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.6e+03  |
| mean 100 episode reward | 154      |
| steps                   | 9.96e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.7e+03  |
| mean 100 episode reward | 163      |
| steps                   | 1.16e+05 |
--------------------------------------
Saving model due to mean reward increase: 169.60000610351562 -> 177.0
Saving model due to mean reward increase: 177.0 -> 183.60000610351562
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.8e+03  |
| mean 100 episode reward | 178      |
| steps                   | 1.34e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 1.9e+03  |
| mean 100 episode reward | 132      |
| steps                   | 1.47e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2e+03    |
| mean 100 episode reward | 153      |
| steps                   | 1.62e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.1e+03  |
| mean 100 episode reward | 87       |
| steps                   | 1.71e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.2e+03  |
| mean 100 episode reward | 31       |
| steps                   | 1.74e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.3e+03  |
| mean 100 episode reward | 59.6     |
| steps                   | 1.8e+05  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.4e+03  |
| mean 100 episode reward | 114      |
| steps                   | 1.91e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.5e+03  |
| mean 100 episode reward | 104      |
| steps                   | 2.02e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.6e+03  |
| mean 100 episode reward | 163      |
| steps                   | 2.18e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.7e+03  |
| mean 100 episode reward | 156      |
| steps                   | 2.34e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.8e+03  |
| mean 100 episode reward | 122      |
| steps                   | 2.46e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 2.9e+03  |
| mean 100 episode reward | 60.7     |
| steps                   | 2.52e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3e+03    |
| mean 100 episode reward | 49.8     |
| steps                   | 2.57e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.1e+03  |
| mean 100 episode reward | 131      |
| steps                   | 2.7e+05  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.2e+03  |
| mean 100 episode reward | 160      |
| steps                   | 2.86e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.3e+03  |
| mean 100 episode reward | 129      |
| steps                   | 2.99e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.4e+03  |
| mean 100 episode reward | 50.4     |
| steps                   | 3.04e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.5e+03  |
| mean 100 episode reward | 131      |
| steps                   | 3.17e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.6e+03  |
| mean 100 episode reward | 153      |
| steps                   | 3.32e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.7e+03  |
| mean 100 episode reward | 169      |
| steps                   | 3.49e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.8e+03  |
| mean 100 episode reward | 180      |
| steps                   | 3.67e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 3.9e+03  |
| mean 100 episode reward | 141      |
| steps                   | 3.81e+05 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 4e+03    |
| mean 100 episode reward | 167      |
| steps                   | 3.98e+05 |
--------------------------------------
Restored model with mean reward: 183.60000610351562
